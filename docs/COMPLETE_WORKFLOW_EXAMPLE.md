# å®Œæ•´å·¥ä½œæµç¤ºä¾‹ï¼šç ”ç©¶ Transformer ä¸ BERT

## ğŸ“– ç ”ç©¶ç›®æ ‡

ç†è§£ Transformer æ¶æ„åŠå…¶åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆBERTï¼‰ä¸­çš„åº”ç”¨

---

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šè·å–è®ºæ–‡ï¼ˆè®ºæ–‡ç®¡ç†ç³»ç»Ÿï¼‰

### ä¸‹è½½ Transformer è®ºæ–‡

```bash
$ python main.py --arxiv 1706.03762

ğŸ” æŸ¥æ‰¾è®ºæ–‡ 1706.03762...
âœ… æ‰¾åˆ°è®ºæ–‡: Attention is All You Need
ğŸ‘¥ ä½œè€…: Ashish Vaswani, et al.
ğŸ“… å¹´ä»½: 2017

â¬‡ï¸  ä¼˜å…ˆä¸‹è½½ TeX æºæ–‡ä»¶...
âœ… TeX æºæ–‡ä»¶å·²ä¸‹è½½
ğŸ“ è§£æ TeX ç»“æ„...
âœ… è§£æå®Œæˆ

ğŸ“ è®ºæ–‡å·²ä¿å­˜åˆ°: papers/1706_03762/
   â”œâ”€â”€ source.tex              (TeXæºæ–‡ä»¶)
   â”œâ”€â”€ paper.pdf               (PDFå¤‡ä»½)
   â”œâ”€â”€ metadata.json           (å…ƒæ•°æ®)
   â””â”€â”€ parsed_structure.json   (è§£æç»“æ„)
```

### ä¸‹è½½ BERT è®ºæ–‡

```bash
$ python main.py --arxiv 1810.04805

ğŸ” æŸ¥æ‰¾è®ºæ–‡ 1810.04805...
âœ… æ‰¾åˆ°è®ºæ–‡: BERT: Pre-training of Deep Bidirectional Transformers
ğŸ‘¥ ä½œè€…: Jacob Devlin, et al.
ğŸ“… å¹´ä»½: 2018

â¬‡ï¸  ä¼˜å…ˆä¸‹è½½ TeX æºæ–‡ä»¶...
âœ… TeX æºæ–‡ä»¶å·²ä¸‹è½½
ğŸ“ è§£æ TeX ç»“æ„...
âœ… è§£æå®Œæˆ

ğŸ“ è®ºæ–‡å·²ä¿å­˜åˆ°: papers/1810_04805/
```

---

## ğŸ’­ ç¬¬äºŒé˜¶æ®µï¼šé˜…è¯» Transformer è®ºæ–‡ï¼ˆæ´å¯Ÿç³»ç»Ÿï¼‰

### å¼€å§‹é˜…è¯»ä¼šè¯

```bash
$ python scripts/insights_cli.py --start-reading 1706_03762

ğŸ“– å¼€å§‹é˜…è¯»ä¼šè¯
   è®ºæ–‡: Attention is All You Need (1706.03762)
   ä¼šè¯ID: session_20260217_0900

ğŸ’¡ æç¤º: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®°å½•æ´å¯Ÿ:
   python scripts/insights_cli.py --insight
```

### é˜…è¯» Introductionï¼Œè®°å½•è§‚å¯Ÿ

```bash
$ python scripts/insights_cli.py --insight

ğŸ“ è®°å½•æ–°æ´å¯Ÿ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ´å¯Ÿå†…å®¹: ç°æœ‰åºåˆ—æ¨¡å‹éƒ½ä¾èµ–å¾ªç¯ç»“æ„ï¼Œé™åˆ¶äº†å¹¶è¡Œè®¡ç®—
æ´å¯Ÿç±»å‹ (observation/question/connection/surprise/critique/insight) [observation]: observation
é‡è¦æ€§ (1-5) [3]: 3
æ‰€åœ¨ç« èŠ‚ [å¯é€‰]: Introduction
ç›¸å…³å¼•ç”¨ [å¯é€‰]:
æ·»åŠ æ ‡ç­¾ [å¯é€‰ï¼Œé€—å·åˆ†éš”]: rnn,limitation

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_001
```

### ç»§ç»­é˜…è¯»ï¼Œå‘ç°æ ¸å¿ƒåˆ›æ–°

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: Self-attentionå¯ä»¥å®Œå…¨æ›¿ä»£å¾ªç¯å’Œå·ç§¯ï¼
æ´å¯Ÿç±»å‹: surprise
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: Introduction
ç›¸å…³å¼•ç”¨: "We propose a new simple network architecture... based solely on attention mechanisms"
æ·»åŠ æ ‡ç­¾: attention,innovation

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_002
```

### é˜…è¯» Model Architectureï¼Œè®°å½•æŠ€æœ¯ç»†èŠ‚

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: Multi-head attentionå…è®¸æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´
æ´å¯Ÿç±»å‹: insight
é‡è¦æ€§: 4
æ‰€åœ¨ç« èŠ‚: 3.2 Multi-Head Attention
ç›¸å…³å¼•ç”¨: "Multi-head attention allows the model to jointly attend to information from different representation subspaces"
æ·»åŠ æ ‡ç­¾: attention,multi-head

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_003
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: Self-attentionçš„å¤æ‚åº¦æ˜¯O(nÂ²Â·d)ï¼Œå…¶ä¸­næ˜¯åºåˆ—é•¿åº¦
æ´å¯Ÿç±»å‹: observation
é‡è¦æ€§: 4
æ‰€åœ¨ç« èŠ‚: 4. Why Self-Attention
æ·»åŠ æ ‡ç­¾: complexity

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_004
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: ä½ç½®ç¼–ç ä¸ºä»€ä¹ˆç”¨sin/coså‡½æ•°ï¼Ÿå›ºå®šçš„è¿˜æ˜¯å¯å­¦ä¹ çš„å¥½ï¼Ÿ
æ´å¯Ÿç±»å‹: question
é‡è¦æ€§: 3
æ‰€åœ¨ç« èŠ‚: 3.5 Positional Encoding
æ·»åŠ æ ‡ç­¾: positional-encoding

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_005
```

### é˜…è¯» Experimentsï¼Œè®°å½•ç»“æœ

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: åœ¨WMTç¿»è¯‘ä»»åŠ¡ä¸Šè¶…è¿‡äº†æ‰€æœ‰ä¹‹å‰çš„æ¨¡å‹
æ´å¯Ÿç±»å‹: observation
é‡è¦æ€§: 4
æ‰€åœ¨ç« èŠ‚: 6. Results
æ·»åŠ æ ‡ç­¾: results,translation

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_006
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: è®­ç»ƒé€Ÿåº¦æ¯”å¾ªç¯æ¨¡å‹å¿«å¾—å¤šï¼Œä½†æ¨ç†æ—¶é•¿åºåˆ—å¯èƒ½æœ‰é—®é¢˜
æ´å¯Ÿç±»å‹: critique
é‡è¦æ€§: 4
æ‰€åœ¨ç« èŠ‚: 6. Results
æ·»åŠ æ ‡ç­¾: efficiency

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_007
```

### ç»“æŸé˜…è¯»ä¼šè¯

```bash
$ python scripts/insights_cli.py --end-reading

ğŸ“Š é˜…è¯»ä¼šè¯ç»“æŸ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“– è®ºæ–‡: Attention is All You Need (1706.03762)
â±ï¸  æ—¶é•¿: 1å°æ—¶23åˆ†é’Ÿ
ğŸ“ æ´å¯Ÿæ•°: 7ä¸ª

æ´å¯Ÿåˆ†å¸ƒ:
  observation: 3ä¸ª (42.9%)
  question:    1ä¸ª (14.3%)
  surprise:    1ä¸ª (14.3%)
  insight:     1ä¸ª (14.3%)
  critique:    1ä¸ª (14.3%)

ç« èŠ‚è¦†ç›–:
  Introduction
  3.2 Multi-Head Attention
  3.5 Positional Encoding
  4. Why Self-Attention
  6. Results

ğŸ’¡ å»ºè®®:
  - é«˜ä»·å€¼æ´å¯Ÿ(4-5åˆ†): 5ä¸ªï¼Œå»ºè®®æç‚¼ä¸ºæƒ³æ³•
  - æœªè§£å†³é—®é¢˜: 1ä¸ªï¼Œå¯ä»¥ä½œä¸ºåç»­ç ”ç©¶æ–¹å‘
```

---

## ğŸ’­ ç¬¬ä¸‰é˜¶æ®µï¼šé˜…è¯» BERT è®ºæ–‡ï¼ˆæ´å¯Ÿç³»ç»Ÿï¼‰

### å¼€å§‹æ–°çš„é˜…è¯»ä¼šè¯

```bash
$ python scripts/insights_cli.py --start-reading 1810_04805

ğŸ“– å¼€å§‹é˜…è¯»ä¼šè¯
   è®ºæ–‡: BERT (1810.04805)
   ä¼šè¯ID: session_20260217_1130
```

### è®°å½•æ´å¯Ÿï¼Œæ³¨æ„ä¸ Transformer çš„è¿æ¥

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: BERTç›´æ¥ä½¿ç”¨Transformerçš„encoderéƒ¨åˆ†
æ´å¯Ÿç±»å‹: connection
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: Introduction
ç›¸å…³å¼•ç”¨: "We use the Transformer encoder architecture"
æ·»åŠ æ ‡ç­¾: bert,transformer,architecture

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_008
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹(ELMo, GPT)éƒ½æ˜¯å•å‘çš„ï¼Œé™åˆ¶äº†è¡¨ç¤ºèƒ½åŠ›
æ´å¯Ÿç±»å‹: observation
é‡è¦æ€§: 4
æ‰€åœ¨ç« èŠ‚: Introduction
æ·»åŠ æ ‡ç­¾: pretraining,limitation

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_009
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: Masked Language Model(MLM)æ˜¯å…³é”®åˆ›æ–°ï¼Œå…è®¸åŒå‘å»ºæ¨¡
æ´å¯Ÿç±»å‹: insight
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: 3.1 Pre-training BERT
ç›¸å…³å¼•ç”¨: "We mask some percentage of the input tokens and predict those masked tokens"
æ·»åŠ æ ‡ç­¾: mlm,bidirectional

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_010
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: Transformerçš„self-attentionå¤©ç„¶æ”¯æŒåŒå‘ä¸Šä¸‹æ–‡
æ´å¯Ÿç±»å‹: insight
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: 3.1
æ·»åŠ æ ‡ç­¾: attention,bidirectional

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_011
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: ä¸ºä»€ä¹ˆmask 15%çš„tokenï¼Ÿè¿™ä¸ªæ¯”ä¾‹æ˜¯æ€ä¹ˆé€‰çš„ï¼Ÿ
æ´å¯Ÿç±»å‹: question
é‡è¦æ€§: 3
æ‰€åœ¨ç« èŠ‚: 3.1
æ·»åŠ æ ‡ç­¾: mlm,hyperparameter

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_012
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: NSP(Next Sentence Prediction)ä»»åŠ¡å¯¹ä¸‹æ¸¸çš„å¥å­å¯¹ä»»åŠ¡æœ‰å¸®åŠ©
æ´å¯Ÿç±»å‹: observation
é‡è¦æ€§: 3
æ‰€åœ¨ç« èŠ‚: 3.2 Pre-training Tasks
æ·»åŠ æ ‡ç­¾: nsp,pretraining

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_013
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: BERTåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°SOTAï¼Œæå‡æƒŠäººï¼
æ´å¯Ÿç±»å‹: surprise
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: 4. Experiments
æ·»åŠ æ ‡ç­¾: results,sota

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_014
```

```bash
$ python scripts/insights_cli.py --insight

æ´å¯Ÿå†…å®¹: åŒå‘é¢„è®­ç»ƒçš„æ•ˆæœæ¯”å•å‘å¥½å¾ˆå¤šï¼Œæ¶ˆèå®éªŒè¯æ˜äº†è¿™ç‚¹
æ´å¯Ÿç±»å‹: insight
é‡è¦æ€§: 5
æ‰€åœ¨ç« èŠ‚: 5.1 Ablation Studies
æ·»åŠ æ ‡ç­¾: bidirectional,ablation

âœ… æ´å¯Ÿå·²åˆ›å»º: insight_015
```

### ç»“æŸ BERT é˜…è¯»ä¼šè¯

```bash
$ python scripts/insights_cli.py --end-reading

ğŸ“Š é˜…è¯»ä¼šè¯ç»“æŸ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“– è®ºæ–‡: BERT (1810.04805)
â±ï¸  æ—¶é•¿: 1å°æ—¶45åˆ†é’Ÿ
ğŸ“ æ´å¯Ÿæ•°: 8ä¸ª

æ´å¯Ÿåˆ†å¸ƒ:
  observation: 2ä¸ª (25.0%)
  connection:  1ä¸ª (12.5%)
  question:    1ä¸ª (12.5%)
  insight:     3ä¸ª (37.5%)
  surprise:    1ä¸ª (12.5%)

ğŸ’¡ å»ºè®®:
  - é«˜ä»·å€¼æ´å¯Ÿ(4-5åˆ†): 6ä¸ª
  - ä¸è®ºæ–‡1706.03762æœ‰è¿æ¥: 2ä¸ªæ´å¯Ÿ
  - å»ºè®®ç”Ÿæˆè·¨è®ºæ–‡æƒ³æ³•
```

---

## ğŸ“ ç¬¬å››é˜¶æ®µï¼šä»æ´å¯Ÿç”Ÿæˆæƒ³æ³•ï¼ˆæ´å¯Ÿç³»ç»Ÿ â†’ æƒ³æ³•ç³»ç»Ÿï¼‰

### æŸ¥çœ‹æ‰€æœ‰æ´å¯Ÿ

```bash
$ python scripts/insights_cli.py --list-insights

ğŸ“‹ æ´å¯Ÿåˆ—è¡¨ (å…±15ä¸ª)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

è®ºæ–‡: 1706.03762 (Attention is All You Need)
  [insight_001] observation (3â­) - ç°æœ‰åºåˆ—æ¨¡å‹éƒ½ä¾èµ–å¾ªç¯ç»“æ„
  [insight_002] surprise    (5â­) - Self-attentionå¯ä»¥å®Œå…¨æ›¿ä»£å¾ªç¯å’Œå·ç§¯ï¼
  [insight_003] insight     (4â­) - Multi-head attentionå…è®¸æ¨¡å‹å…³æ³¨ä¸åŒè¡¨ç¤ºå­ç©ºé—´
  [insight_004] observation (4â­) - Self-attentionçš„å¤æ‚åº¦æ˜¯O(nÂ²Â·d)
  [insight_005] question    (3â­) - ä½ç½®ç¼–ç ä¸ºä»€ä¹ˆç”¨sin/coså‡½æ•°ï¼Ÿ
  [insight_006] observation (4â­) - åœ¨WMTç¿»è¯‘ä»»åŠ¡ä¸Šè¶…è¿‡äº†æ‰€æœ‰ä¹‹å‰çš„æ¨¡å‹
  [insight_007] critique    (4â­) - è®­ç»ƒé€Ÿåº¦å¿«ä½†æ¨ç†æ—¶é•¿åºåˆ—å¯èƒ½æœ‰é—®é¢˜

è®ºæ–‡: 1810.04805 (BERT)
  [insight_008] connection  (5â­) - BERTç›´æ¥ä½¿ç”¨Transformerçš„encoderéƒ¨åˆ†
  [insight_009] observation (4â­) - ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹éƒ½æ˜¯å•å‘çš„
  [insight_010] insight     (5â­) - MLMå…è®¸åŒå‘å»ºæ¨¡
  [insight_011] insight     (5â­) - Transformerçš„self-attentionå¤©ç„¶æ”¯æŒåŒå‘
  [insight_012] question    (3â­) - ä¸ºä»€ä¹ˆmask 15%çš„tokenï¼Ÿ
  [insight_013] observation (3â­) - NSPä»»åŠ¡å¯¹å¥å­å¯¹ä»»åŠ¡æœ‰å¸®åŠ©
  [insight_014] surprise    (5â­) - BERTåœ¨11ä¸ªä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°SOTA
  [insight_015] insight     (5â­) - åŒå‘é¢„è®­ç»ƒæ•ˆæœæ¯”å•å‘å¥½å¾ˆå¤š
```

### ç”Ÿæˆæƒ³æ³•ï¼ˆè‡ªåŠ¨å»ºè®®ï¼‰

```bash
$ python scripts/insights_cli.py --gen-ideas --auto

ğŸ’¡ åˆ†ææ´å¯Ÿï¼Œç”Ÿæˆæƒ³æ³•å»ºè®®...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

å»ºè®®çš„æƒ³æ³•ç»„åˆ:

ã€ç»„åˆ1ã€‘Transformerçš„æ ¸å¿ƒåˆ›æ–°
  ç›¸å…³æ´å¯Ÿ:
    - insight_002 (5â­): Self-attentionå¯ä»¥å®Œå…¨æ›¿ä»£å¾ªç¯
    - insight_003 (4â­): Multi-head attentionçš„è¡¨ç¤ºèƒ½åŠ›
    - insight_006 (4â­): åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„çªç ´
  ç›¸ä¼¼åº¦: 85%
  ä¸»é¢˜: transformer, attention, innovation

ã€ç»„åˆ2ã€‘BERTçš„åŒå‘é¢„è®­ç»ƒçªç ´
  ç›¸å…³æ´å¯Ÿ:
    - insight_010 (5â­): MLMå…è®¸åŒå‘å»ºæ¨¡
    - insight_011 (5â­): Transformeræ”¯æŒåŒå‘
    - insight_015 (5â­): åŒå‘æ¯”å•å‘æ•ˆæœå¥½
  ç›¸ä¼¼åº¦: 92%
  ä¸»é¢˜: bert, bidirectional, pretraining

ã€ç»„åˆ3ã€‘Transformeråˆ°BERTçš„æ¼”è¿›ï¼ˆè·¨è®ºæ–‡ï¼‰
  ç›¸å…³æ´å¯Ÿ:
    - insight_002 (5â­): Self-attentionçš„åˆ›æ–° [è®ºæ–‡A]
    - insight_008 (5â­): BERTåŸºäºTransformer [è®ºæ–‡B]
    - insight_011 (5â­): Transformeræ”¯æŒåŒå‘ [è®ºæ–‡B]
  ç›¸ä¼¼åº¦: 88%
  ä¸»é¢˜: transformer, bert, architecture
  ç‰¹ç‚¹: è·¨è®ºæ–‡è¿æ¥

é€‰æ‹©è¦ç”Ÿæˆçš„ç»„åˆ (1-3ï¼Œé€—å·åˆ†éš”ï¼Œæˆ–'all'): all
```

### ç”Ÿæˆæƒ³æ³• 1

```bash
ğŸ“ ç”Ÿæˆæƒ³æ³•: Transformerçš„æ ¸å¿ƒåˆ›æ–°
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æƒ³æ³•æ ‡é¢˜: Self-Attentionæœºåˆ¶çš„çªç ´æ€§åˆ›æ–°
æƒ³æ³•å†…å®¹:
  Transformerè®ºæ–‡æå‡ºäº†é©å‘½æ€§çš„Self-Attentionæœºåˆ¶ï¼Œå®Œå…¨æŠ›å¼ƒäº†
  ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚Multi-head attentionå…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨
  åºåˆ—ä¸­ä¸åŒä½ç½®çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´ï¼Œæå¤§å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
  åœ¨WMTç¿»è¯‘ä»»åŠ¡ä¸Šï¼ŒTransformerè¶…è¿‡äº†æ‰€æœ‰ä¹‹å‰çš„æ¨¡å‹ï¼Œè¯æ˜äº†
  çº¯æ³¨æ„åŠ›æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚

åŸºäºæ´å¯Ÿ: insight_002, insight_003, insight_006
ä¸»è¦è®ºæ–‡: 1706_03762
ç½®ä¿¡åº¦: 0.9

âœ… æƒ³æ³•å·²åˆ›å»º: idea_from_insights_001
```

### ç”Ÿæˆæƒ³æ³• 2

```bash
ğŸ“ ç”Ÿæˆæƒ³æ³•: BERTçš„åŒå‘é¢„è®­ç»ƒçªç ´
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æƒ³æ³•æ ‡é¢˜: BERTé€šè¿‡MLMå®ç°çœŸæ­£çš„åŒå‘é¢„è®­ç»ƒ
æƒ³æ³•å†…å®¹:
  BERTçš„æ ¸å¿ƒåˆ›æ–°æ˜¯Masked Language Modelé¢„è®­ç»ƒä»»åŠ¡ï¼Œé€šè¿‡éšæœºmask
  è¾“å…¥tokenå¹¶é¢„æµ‹å®ƒä»¬ï¼Œå®ç°äº†çœŸæ­£çš„åŒå‘è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚è¿™ä¸€æ–¹æ³•
  å……åˆ†åˆ©ç”¨äº†Transformeræ¶æ„ä¸­self-attentionå¤©ç„¶æ”¯æŒåŒå‘ä¸Šä¸‹æ–‡çš„
  ç‰¹æ€§ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒåŒå‘é¢„è®­ç»ƒç›¸æ¯”å•å‘æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

åŸºäºæ´å¯Ÿ: insight_010, insight_011, insight_015
ä¸»è¦è®ºæ–‡: 1810_04805
ç½®ä¿¡åº¦: 0.95

âœ… æƒ³æ³•å·²åˆ›å»º: idea_from_insights_002
```

### ç”Ÿæˆæƒ³æ³• 3ï¼ˆè·¨è®ºæ–‡ï¼‰

```bash
ğŸ“ ç”Ÿæˆæƒ³æ³•: Transformeråˆ°BERTçš„æ¼”è¿›
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æƒ³æ³•æ ‡é¢˜: Transformeræ¶æ„æ˜¯BERTåŒå‘é¢„è®­ç»ƒçš„åŸºç¡€
æƒ³æ³•å†…å®¹:
  Transformerçš„self-attentionæœºåˆ¶ä¸ä»…æ›¿ä»£äº†å¾ªç¯ç»“æ„ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œ
  å®ƒå¤©ç„¶æ”¯æŒåŒå‘çš„ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚BERTæ­£æ˜¯åŸºäºè¿™ä¸€ç‰¹æ€§ï¼Œé€šè¿‡åœ¨
  Transformer encoderä¸Šåº”ç”¨MLMé¢„è®­ç»ƒï¼Œå®ç°äº†çœŸæ­£çš„åŒå‘è¯­è¨€ç†è§£ã€‚
  è¿™ä¸ªä¾‹å­å±•ç¤ºäº†æ¶æ„åˆ›æ–°å¦‚ä½•ä¸ºé¢„è®­ç»ƒç­–ç•¥åˆ›æ–°é“ºå¹³é“è·¯ã€‚

åŸºäºæ´å¯Ÿ: insight_002, insight_008, insight_011
ç›¸å…³è®ºæ–‡: 1706_03762, 1810_04805
ç½®ä¿¡åº¦: 0.88
ç±»å‹: è·¨è®ºæ–‡ç»¼åˆ

âœ… æƒ³æ³•å·²åˆ›å»º: idea_from_insights_003
```

---

## ğŸ“ ç¬¬äº”é˜¶æ®µï¼šåˆ›å»ºç»“æ„åŒ–æƒ³æ³•ï¼ˆç»“æ„åŒ–æƒ³æ³•ç³»ç»Ÿï¼‰

### å°†æ´å¯Ÿæƒ³æ³•è½¬æ¢ä¸ºåŸå­æƒ³æ³•

```bash
$ python scripts/structured_ideas_cli.py --from-insight idea_from_insights_001

ğŸ“ ä»æ´å¯Ÿæƒ³æ³•åˆ›å»ºç»“æ„åŒ–æƒ³æ³•
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æºæƒ³æ³•: Self-Attentionæœºåˆ¶çš„çªç ´æ€§åˆ›æ–°
ç±»å‹: åŸå­æƒ³æ³•

âš ï¸  ç»“æ„åŒ–æƒ³æ³•éœ€è¦ç²¾ç¡®çš„å¼•ç”¨ä¿¡æ¯

æ·»åŠ æ¥æº 1:
  è®ºæ–‡ID: 1706_03762
  ç« èŠ‚: 3. Model Architecture
  å­ç« èŠ‚: 3.2 Attention
  é¡µç : 4
  å¼•ç”¨æ–‡æœ¬: "An attention function can be described as mapping a query and a set of key-value pairs to an output"
  ç¬”è®°: Self-attentionçš„æ ¸å¿ƒå®šä¹‰

æ˜¯å¦æ·»åŠ æ›´å¤šæ¥æº? (y/N): y

æ·»åŠ æ¥æº 2:
  è®ºæ–‡ID: 1706_03762
  ç« èŠ‚: 3. Model Architecture
  å­ç« èŠ‚: 3.2.2 Multi-Head Attention
  é¡µç : 5
  å¼•ç”¨æ–‡æœ¬: "Multi-head attention allows the model to jointly attend to information from different representation subspaces"
  ç¬”è®°: Multi-headæœºåˆ¶çš„ä¼˜åŠ¿

æ˜¯å¦æ·»åŠ æ›´å¤šæ¥æº? (y/N): n

âœ… åŸå­æƒ³æ³•å·²åˆ›å»º: atomic_001
   æ ‡é¢˜: Self-Attentionæœºåˆ¶çš„çªç ´æ€§åˆ›æ–°
   æ¥æº: 2ä¸ªå¼•ç”¨
   ç›¸å…³æ´å¯Ÿ: insight_002, insight_003, insight_006
```

### åˆ›å»ºç¬¬äºŒä¸ªåŸå­æƒ³æ³•

```bash
$ python scripts/structured_ideas_cli.py --from-insight idea_from_insights_002

æ·»åŠ æ¥æº 1:
  è®ºæ–‡ID: 1810_04805
  ç« èŠ‚: 3. BERT
  å­ç« èŠ‚: 3.1 Pre-training BERT
  é¡µç : 3
  å¼•ç”¨æ–‡æœ¬: "In order to train a deep bidirectional representation, we mask some percentage of the input tokens at random"
  ç¬”è®°: MLMé¢„è®­ç»ƒæ–¹æ³•

æ·»åŠ æ¥æº 2:
  è®ºæ–‡ID: 1810_04805
  ç« èŠ‚: 5. Ablation Studies
  å­ç« èŠ‚: 5.1 Effect of Pre-training Tasks
  é¡µç : 8
  å¼•ç”¨æ–‡æœ¬: "Removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD"
  ç¬”è®°: åŒå‘é¢„è®­ç»ƒçš„æ•ˆæœéªŒè¯

âœ… åŸå­æƒ³æ³•å·²åˆ›å»º: atomic_002
   æ ‡é¢˜: BERTé€šè¿‡MLMå®ç°çœŸæ­£çš„åŒå‘é¢„è®­ç»ƒ
   æ¥æº: 2ä¸ªå¼•ç”¨
```

---

## ğŸ”— ç¬¬å…­é˜¶æ®µï¼šåˆ›å»ºç»„åˆæƒ³æ³•

### åˆ›å»ºè·¨è®ºæ–‡çš„ç»„åˆæƒ³æ³•

```bash
$ python scripts/structured_ideas_cli.py --composite

ğŸ“ åˆ›å»ºç»„åˆæƒ³æ³•
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

é€‰æ‹©çˆ¶æƒ³æ³•:
  1. [atomic_001] Self-Attentionæœºåˆ¶çš„çªç ´æ€§åˆ›æ–°
  2. [atomic_002] BERTé€šè¿‡MLMå®ç°çœŸæ­£çš„åŒå‘é¢„è®­ç»ƒ

çˆ¶æƒ³æ³•ID (é€—å·åˆ†éš”): 1,2

æƒ³æ³•1çš„å…³ç³»ç±»å‹:
  - extends: æƒ³æ³•1è¢«æ‰©å±•
  - combines: æƒ³æ³•1è¢«ç»„åˆ
é€‰æ‹© (extends/combines): extends

æƒ³æ³•2çš„å…³ç³»ç±»å‹:
  - combines: æƒ³æ³•2è¢«ç»„åˆ
  - refines: æƒ³æ³•2ç²¾åŒ–äº†æƒ³æ³•1
é€‰æ‹© (combines/refines): combines

ç»„åˆæƒ³æ³•æ ‡é¢˜: Transformeræ¶æ„ä¿ƒæˆBERTçš„åŒå‘é¢„è®­ç»ƒåˆ›æ–°
ç»„åˆæƒ³æ³•å†…å®¹:
  Transformerçš„self-attentionæœºåˆ¶ä¸ä»…å®ç°äº†å¹¶è¡Œè®¡ç®—ï¼Œæ›´é‡è¦çš„æ˜¯
  æä¾›äº†å¤©ç„¶çš„åŒå‘å»ºæ¨¡èƒ½åŠ›ã€‚BERTå……åˆ†åˆ©ç”¨è¿™ä¸€æ¶æ„ä¼˜åŠ¿ï¼Œé€šè¿‡MLM
  é¢„è®­ç»ƒä»»åŠ¡å®ç°äº†çœŸæ­£çš„åŒå‘è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚è¿™å±•ç¤ºäº†åŸºç¡€æ¶æ„åˆ›æ–°
  å¦‚ä½•ä¸ºä¸Šå±‚æ–¹æ³•åˆ›æ–°åˆ›é€ å¯èƒ½æ€§ã€‚

âš ï¸  é‡è¦æç¤º:
ç»„åˆæƒ³æ³•å»ºè®®æä¾›æ–°çš„æ¥æºæ¥æ”¯æŒç»„åˆé€»è¾‘ã€‚
ä¾‹å¦‚ï¼šè¯´æ˜'ä¸ºä»€ä¹ˆè¿™æ ·ç»„åˆ'çš„è®ºæ–‡ç« èŠ‚ã€‚

æ˜¯å¦æ·»åŠ æ–°æ¥æºï¼ˆæ”¯æŒç»„åˆé€»è¾‘ï¼‰ï¼Ÿ(æ¨è: y/N): y

æ–°æ¥æº 1ï¼ˆç»„åˆé€»è¾‘æ”¯æŒï¼‰:
  è®ºæ–‡ID: 1810_04805
  ç« èŠ‚: Introduction
  é¡µç : 1
  å¼•ç”¨æ–‡æœ¬: "We demonstrate the importance of bidirectional pre-training for language representations... the Transformer encoder allows us to do so"
  ç¬”è®°: BERTè®ºæ–‡æ˜ç¡®è¯´æ˜Transformer encoderä½¿å¾—åŒå‘é¢„è®­ç»ƒæˆä¸ºå¯èƒ½
  ç›®çš„: combination_logic

âœ… ç»„åˆæƒ³æ³•å·²åˆ›å»º: composite_001
   çˆ¶æƒ³æ³•: 2ä¸ª (atomic_001 extends, atomic_002 combines)
   æ¥æº: 5ä¸ª (4ä¸ªç»§æ‰¿ + 1ä¸ªæ–°æ¥æº)
   å…³ç³»: è·¨è®ºæ–‡ç»„åˆ
```

---

## ğŸ“Š ç¬¬ä¸ƒé˜¶æ®µï¼šå¯è§†åŒ–å’Œåˆ†æ

### æŸ¥çœ‹æƒ³æ³•ç½‘ç»œ

```bash
$ python scripts/structured_ideas_cli.py --visualize

ğŸ¨ ç”Ÿæˆæƒ³æ³•ç½‘ç»œå¯è§†åŒ–...

æƒ³æ³•ç½‘ç»œ:
  èŠ‚ç‚¹æ•°: 3
  è¾¹æ•°: 2

ä¿å­˜ä½ç½®:
  - knowledge/ideas/idea_network.png
  - knowledge/ideas/idea_network.json

æµè§ˆå™¨æ‰“å¼€...
```

### å¯è§†åŒ–å›¾ç¤º

```
        atomic_001
    (Self-Attentionåˆ›æ–°)
             â”‚
             â”‚ extends
             â†“
       composite_001
(Transformerä¿ƒæˆBERTåˆ›æ–°)
             â†‘
             â”‚ combines
             â”‚
        atomic_002
     (BERTåŒå‘é¢„è®­ç»ƒ)
```

### ç»Ÿè®¡åˆ†æ

```bash
$ python scripts/insights_cli.py --stats

ğŸ“Š å®Œæ•´ç ”ç©¶ç»Ÿè®¡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ è®ºæ–‡ç®¡ç†:
  - è®ºæ–‡æ€»æ•°: 2ç¯‡
  - TeXæºæ–‡ä»¶: 2ç¯‡ (100%)
  - æ€»é¡µæ•°: 25é¡µ

ğŸ’­ æ´å¯Ÿç³»ç»Ÿ:
  - æ´å¯Ÿæ€»æ•°: 15ä¸ª
  - é˜…è¯»ä¼šè¯: 2æ¬¡
  - æ€»é˜…è¯»æ—¶é•¿: 3å°æ—¶8åˆ†é’Ÿ
  - å¹³å‡æ´å¯Ÿ/è®ºæ–‡: 7.5ä¸ª
  - å·²è½¬æ¢ä¸ºæƒ³æ³•: 15ä¸ª (100%)

ğŸ’¡ æƒ³æ³•ç”Ÿæˆ:
  - æ´å¯Ÿæƒ³æ³•: 3ä¸ª
  - ç»“æ„åŒ–æƒ³æ³•: 3ä¸ª
    - åŸå­æƒ³æ³•: 2ä¸ª
    - ç»„åˆæƒ³æ³•: 1ä¸ª

ğŸ¯ æ•ˆç‡æŒ‡æ ‡:
  - æ´å¯Ÿâ†’æƒ³æ³•è½¬åŒ–ç‡: 100%
  - å¹³å‡é˜…è¯»æ—¶é•¿: 94åˆ†é’Ÿ/è®ºæ–‡
  - è·¨è®ºæ–‡è¿æ¥: 1ä¸ª
```

### è¿½è¸ªç‰¹å®šæ´å¯Ÿçš„æ¼”åŒ–

```bash
$ python scripts/insights_cli.py --trace insight_011

ğŸ“ æ´å¯Ÿè¿½è¸ª
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

åŸå§‹æ´å¯Ÿ:
  ID: insight_011
  å†…å®¹: Transformerçš„self-attentionå¤©ç„¶æ”¯æŒåŒå‘ä¸Šä¸‹æ–‡
  ç±»å‹: insight
  é‡è¦æ€§: 5â­
  æ¥æº: 1810.04805, ç« èŠ‚3.1
  åˆ›å»ºæ—¶é—´: 2026-02-17 11:45

æ¼”åŒ–è·¯å¾„:
  â†“ ç»„åˆè¿›å…¥
  idea_from_insights_002
  "BERTé€šè¿‡MLMå®ç°çœŸæ­£çš„åŒå‘é¢„è®­ç»ƒ"

  â†“ è½¬æ¢ä¸º
  atomic_002
  (ç»“æ„åŒ–åŸå­æƒ³æ³•ï¼ŒåŒ…å«ç²¾ç¡®å¼•ç”¨)

  â†“ ä½œä¸ºçˆ¶æƒ³æ³•
  composite_001
  "Transformeræ¶æ„ä¿ƒæˆBERTçš„åŒå‘é¢„è®­ç»ƒåˆ›æ–°"

å½±å“èŒƒå›´:
  - ç›´æ¥è¡ç”Ÿæƒ³æ³•: 3ä¸ª
  - é—´æ¥å½±å“æƒ³æ³•: 1ä¸ª
  - è¢«å¼•ç”¨æ¬¡æ•°: 4æ¬¡
```

---

## ğŸ’¾ ç¬¬å…«é˜¶æ®µï¼šå¯¼å‡ºå’Œåˆ†äº«

### å¯¼å‡ºç ”ç©¶æŠ¥å‘Š

```bash
$ python scripts/export.py --format markdown --output bert_research.md

ğŸ“ ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...

åŒ…å«å†…å®¹:
  âœ… è®ºæ–‡æ‘˜è¦ (2ç¯‡)
  âœ… é˜…è¯»æ´å¯Ÿ (15ä¸ª)
  âœ… æ´å¯Ÿæƒ³æ³• (3ä¸ª)
  âœ… ç»“æ„åŒ–æƒ³æ³• (3ä¸ª)
  âœ… æƒ³æ³•å…³ç³»å›¾
  âœ… å¼•ç”¨æ¥æºåˆ—è¡¨

âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: bert_research.md
```

### æŠ¥å‘Šå†…å®¹ç¤ºä¾‹

```markdown
# BERTç ”ç©¶æŠ¥å‘Š

## ç ”ç©¶æ¦‚è¿°

æœ¬ç ”ç©¶æ·±å…¥åˆ†æäº†Transformeræ¶æ„åŠå…¶åœ¨BERTé¢„è®­ç»ƒä¸­çš„åº”ç”¨ã€‚

## æ ¸å¿ƒè®ºæ–‡

### 1. Attention is All You Need (1706.03762)
- **ä½œè€…**: Ashish Vaswani, et al.
- **å¹´ä»½**: 2017
- **æ ¸å¿ƒè´¡çŒ®**: æå‡ºçº¯attentionæ¶æ„
- **å…³é”®æ´å¯Ÿ**: 7ä¸ª

### 2. BERT (1810.04805)
- **ä½œè€…**: Jacob Devlin, et al.
- **å¹´ä»½**: 2018
- **æ ¸å¿ƒè´¡çŒ®**: åŒå‘é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- **å…³é”®æ´å¯Ÿ**: 8ä¸ª

## æ ¸å¿ƒæ´å¯Ÿ

### Transformeråˆ›æ–°
1. Self-attentionå¯ä»¥å®Œå…¨æ›¿ä»£å¾ªç¯å’Œå·ç§¯
2. Multi-head attentionæä¾›å¤šå­ç©ºé—´è¡¨ç¤º
3. åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°SOTA

### BERTçªç ´
1. MLMå®ç°çœŸæ­£çš„åŒå‘å»ºæ¨¡
2. Transformeræ¶æ„å¤©ç„¶æ”¯æŒåŒå‘
3. åŒå‘é¢„è®­ç»ƒæ•ˆæœæ˜¾è‘—ä¼˜äºå•å‘

## ç»“æ„åŒ–æƒ³æ³•

### åŸå­æƒ³æ³•1: Self-Attentionæœºåˆ¶çš„çªç ´æ€§åˆ›æ–°
**æ¥æº**:
- [1706.03762] 3.2 Attention, p.4
- [1706.03762] 3.2.2 Multi-Head Attention, p.5

### åŸå­æƒ³æ³•2: BERTé€šè¿‡MLMå®ç°åŒå‘é¢„è®­ç»ƒ
**æ¥æº**:
- [1810.04805] 3.1 Pre-training BERT, p.3
- [1810.04805] 5.1 Ablation Studies, p.8

### ç»„åˆæƒ³æ³•: Transformerä¿ƒæˆBERTåˆ›æ–°
**çˆ¶æƒ³æ³•**: atomic_001 (extends), atomic_002 (combines)
**æ–°æ¥æº**: [1810.04805] Introduction, p.1

## æƒ³æ³•ç½‘ç»œ

[å›¾ç‰‡: idea_network.png]

## ç ”ç©¶ç»“è®º

Transformerçš„self-attentionæœºåˆ¶ä¸ºåŒå‘é¢„è®­ç»ƒåˆ›é€ äº†æ¶æ„åŸºç¡€ï¼Œ
BERTé€šè¿‡MLMå……åˆ†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œå®ç°äº†è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„é‡å¤§çªç ´ã€‚
```

---

## ğŸ¯ å·¥ä½œæµæ€»ç»“

### æ—¶é—´æŠ•å…¥

```
é˜¶æ®µ1 - ä¸‹è½½è®ºæ–‡:        10åˆ†é’Ÿ
é˜¶æ®µ2 - é˜…è¯»Transformer:  1å°æ—¶23åˆ†é’Ÿ
é˜¶æ®µ3 - é˜…è¯»BERT:         1å°æ—¶45åˆ†é’Ÿ
é˜¶æ®µ4 - ç”Ÿæˆæƒ³æ³•:        15åˆ†é’Ÿ
é˜¶æ®µ5 - ç»“æ„åŒ–æƒ³æ³•:      30åˆ†é’Ÿ
é˜¶æ®µ6 - ç»„åˆæƒ³æ³•:        20åˆ†é’Ÿ
é˜¶æ®µ7 - å¯è§†åŒ–åˆ†æ:      10åˆ†é’Ÿ
é˜¶æ®µ8 - å¯¼å‡ºæŠ¥å‘Š:        5åˆ†é’Ÿ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡:                    4å°æ—¶38åˆ†é’Ÿ
```

### äº§å‡ºç»Ÿè®¡

```
ğŸ“„ è®ºæ–‡: 2ç¯‡
ğŸ’­ æ´å¯Ÿ: 15ä¸ª
ğŸ’¡ æƒ³æ³•: 6ä¸ª (3ä¸ªæ´å¯Ÿæƒ³æ³• + 3ä¸ªç»“æ„åŒ–æƒ³æ³•)
ğŸ“š å¼•ç”¨: 5ä¸ªç²¾ç¡®å¼•ç”¨
ğŸ”— å…³ç³»: 2æ¡æƒ³æ³•è¿æ¥
ğŸ“Š æŠ¥å‘Š: 1ä»½ç ”ç©¶æŠ¥å‘Š
```

### å…³é”®æ”¶è·

1. **æ¸è¿›å¼å·¥ä½œæµ**
   - é˜…è¯» â†’ æ´å¯Ÿ â†’ æƒ³æ³• â†’ ç»“æ„åŒ–
   - æ¯æ­¥éƒ½è½»é‡ï¼Œä¸é€ æˆè´Ÿæ‹…

2. **å®Œæ•´çš„å¯è¿½æº¯æ€§**
   - ä»è®ºæ–‡ â†’ æ´å¯Ÿ â†’ æƒ³æ³• â†’ å¼•ç”¨
   - å¯ä»¥è¿½è¸ªä»»ä½•æƒ³æ³•çš„æ¥æº

3. **è·¨è®ºæ–‡è¿æ¥**
   - å‘ç°è®ºæ–‡é—´çš„å…³ç³»
   - å½¢æˆç»¼åˆæ€§ç†è§£

4. **å­¦æœ¯ä¸¥è°¨æ€§**
   - æ‰€æœ‰ç»“æ„åŒ–æƒ³æ³•éƒ½æœ‰ç²¾ç¡®å¼•ç”¨
   - ç»„åˆæƒ³æ³•æœ‰æ–°æ¥æºæ”¯æŒ

---

## ğŸš€ ä¸‹ä¸€æ­¥å¯èƒ½çš„æ–¹å‘

### æ‰©å±•ç ”ç©¶

```bash
# é˜…è¯»æ›´å¤šç›¸å…³è®ºæ–‡
python main.py --arxiv 1910.10683  # GPT-2
python main.py --arxiv 1801.06146  # ELMo

# ç»§ç»­è®°å½•æ´å¯Ÿ
python scripts/insights_cli.py --start-reading 1910_10683

# å»ºç«‹æ›´å¤šè·¨è®ºæ–‡è¿æ¥
python scripts/insights_cli.py --gen-ideas --auto
```

### æ·±åŒ–åˆ†æ

```bash
# åˆ›å»ºæ›´å¤šç»„åˆæƒ³æ³•
python scripts/structured_ideas_cli.py --composite

# åˆ†ææƒ³æ³•ç½‘ç»œ
python scripts/structured_ideas_cli.py --analyze-network

# è¯†åˆ«ç ”ç©¶gap
python scripts/structured_ideas_cli.py --find-gaps
```

### è¾“å‡ºåº”ç”¨

```bash
# ç”Ÿæˆæ–‡çŒ®ç»¼è¿°è‰ç¨¿
python scripts/export.py --format literature-review

# ç”Ÿæˆç ”ç©¶ææ¡ˆ
python scripts/export.py --format proposal

# ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿
python scripts/export.py --format slides
```

---

## ğŸ’¡ æ€»ç»“

è¿™ä¸ªå®Œæ•´çš„å·¥ä½œæµå±•ç¤ºäº†å››ä¸ªå­ç³»ç»Ÿå¦‚ä½•ååŒå·¥ä½œï¼š

1. **è®ºæ–‡ç®¡ç†ç³»ç»Ÿ**: æä¾›é«˜è´¨é‡è®ºæ–‡æºï¼ˆTeXä¼˜å…ˆï¼‰
2. **æ´å¯Ÿç³»ç»Ÿ**: é™ä½è®°å½•é—¨æ§›ï¼Œæ•è·é˜…è¯»æ—¶çš„æƒ³æ³•
3. **æƒ³æ³•ç³»ç»Ÿ**: çµæ´»è¿­ä»£ï¼Œä»ç¢ç‰‡åˆ°ç»“æ„åŒ–
4. **ç»“æ„åŒ–ç³»ç»Ÿ**: å­¦æœ¯ä¸¥è°¨ï¼Œå½¢æˆå¼•ç”¨ç½‘ç»œ

**å…³é”®åŸåˆ™**:
- ğŸ¯ **æ¸è¿›å¼**: ä»ç²—ç³™åˆ°ç²¾ç»†ï¼Œä¸è¦ä¸€æ¬¡åˆ°ä½
- ğŸ“ **ä½é—¨æ§›**: éšæ—¶è®°å½•ï¼Œä¸è¦æœ‰å¿ƒç†è´Ÿæ‹…
- ğŸ”— **å¯è¿½æº¯**: ä¿ç•™å®Œæ•´çš„æ€è€ƒè½¨è¿¹
- ğŸ“ **ä¸¥è°¨æ€§**: æœ€ç»ˆå½¢æˆå­¦æœ¯çº§è¾“å‡º

äº«å—ç ”ç©¶çš„è¿‡ç¨‹ï¼ğŸ‰
